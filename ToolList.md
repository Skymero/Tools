# Tools Directory Overview

| Directory Name | Purpose | Completed |
|---------------|---------|:---------:|
| WebConquest |Google Maps business scraper with website detection | □ |
| BizFinancialTrackingTool | Business financial tracking and management | □ |
| PlantUML_2_Diag | Convert PlantUML to diagrams or visualization | □ |
| SuccessEval | Success evaluation metrics or reporting tool | □ |
| Tapa_tool | Unknown specific purpose tool | □ |
| WebCodeExtractor | Extract code elements from web pages | □ |
| emf_calculator | EMF (Electromagnetic Field) calculator | □ |
| Lavoe | Song Analysis tool | □ |


# WebConquest

**Project Description:**
WebConquest is a Python automation tool that streamlines the process of collecting business information from Google Maps. It automates searches for businesses in a specified area, scrapes key contact details (such as business names, addresses, and websites), and organizes results into CSV files based on the presence of a website. This project highlights skills in web automation, data extraction, and data processing, making it relevant for engineering roles focused on Python, automation, or data engineering.

**Libraries Used:**
- `selenium`: Automates browser interactions for navigating and scraping dynamic web pages.
- `beautifulsoup4`: Parses and extracts structured data from HTML content.


# LinkedInNetworkScraper

**Project Description:**
LinkedInNetworkScraper is a Python automation tool that extracts career data from LinkedIn users in my network. It automates the login process, loads and parses all user connections, and scrapes detailed experience information from each connection’s profile. The results are organized and exported to CSV files for further analysis. This project demonstrates expertise in browser automation, web scraping, and structured data extraction from dynamic web platforms—skills highly relevant for engineering roles involving Python and automation.

**Libraries Used:**
- `selenium`: Automates browser actions for logging in, navigating, and interacting with LinkedIn pages.
- `webdriver-manager`: Manages and installs the appropriate ChromeDriver for Selenium.
- `beautifulsoup4`: Parses HTML content to extract profile and experience information.


# Lavoe

**Project Description:**  
Lavoe is an advanced audio analysis tool designed to extract detailed musical and acoustic characteristics from audio files, note by note. It provides musicians, producers, and researchers with a comprehensive breakdown of sound elements using both classical signal processing and modern machine learning. Key features include pitch and note detection, instrument classification, timbre and harmonic analysis, envelope (ADSR) extraction, vibrato and reverb analysis, distortion and filtering detection, dynamics, articulation, and texture analysis. The project demonstrates expertise in digital signal processing, machine learning, and audio feature engineering.

**Libraries Used:**  
- `numpy`, `scipy`, `pandas`, `matplotlib`: Core scientific computing and data analysis.
- `librosa`, `aubio`, `essentia`, `pydub`, `soundfile`: Audio processing and feature extraction.
- `crepe`: Deep learning-based pitch detection.
- `scikit-learn`, `torch`: Machine learning and neural network modeling.
- `tqdm`: Progress visualization for data processing pipelines.


# Tapa_tool

**Project Description:**  
Tapa_tool is a specialized automation tool designed to help engineers and procurement professionals find electronic parts that comply with Trade Agreements Act (TAA) requirements. The tool extracts part requirements from documents, searches multiple electronics distributor websites (such as Mouser, Digi-Key, Arrow, and others) for matching parts, and checks each part’s manufacturing information for TAA compliance. Results are presented in a user-friendly interface, with direct links to compliant parts. This project demonstrates expertise in web scraping, requirements extraction, compliance verification, and workflow automation for regulated supply chains.

**Libraries Used:**  
- `requests`, `beautifulsoup4`: For web scraping and HTML parsing.
- `pandas`: For organizing and processing tabular data.
- `selenium`: For interacting with dynamic web pages (if required).
- `json`: For handling structured data.
- `logging`: For robust process tracking and error handling.


# PlantUML_2_Diag

**Project Description:**  
PlantUML_2_Diag is a Python-based tool that converts PlantUML activity diagrams into high-quality visual diagrams using the Graphviz library. By parsing PlantUML files, the tool automatically extracts activities and their sequence, then generates a directed graph (e.g., PNG) for easy visualization and documentation. This project demonstrates skills in parsing, automation, and programmatic diagram generation, which are valuable for engineering roles involving workflow analysis, documentation, and automation.

**Libraries Used:**  
- `graphviz`: For programmatically generating and rendering diagrams.
- `re`: For parsing and extracting activities from PlantUML files.
- `sys`, `os`: For file handling and command-line automation.


# GrocerySaver

**Project Description:**  
GrocerySaver is a Python automation tool designed to help users compare grocery prices across multiple online stores. The tool reads a list of grocery items, automates the process of searching for each item on different retailer websites, scrapes the prices using browser automation and HTML parsing, and compiles the results into a CSV file for easy comparison. This project demonstrates expertise in web automation, data extraction, and workflow optimization—skills relevant for engineering roles involving Python, automation, and data processing.

**Libraries Used:**  
- `selenium`: Automates browser actions for navigating and scraping dynamic web pages.
- `webdriver-manager`: Manages and installs the appropriate ChromeDriver for Selenium.
- `beautifulsoup4`: Parses HTML content to extract price information.
- `re`, `csv`, `time`: Standard Python libraries for pattern matching, file handling, and timing.

